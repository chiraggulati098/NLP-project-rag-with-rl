{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "First, we need to import the necessary libraries and set up the environment. We will be using HuggingFace Models hosted under **Nebius** platform. Obviously, you can use your own models as long as they are compatible with OpenAI's API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import dotenv\n",
    "dotenv.load_dotenv()\n",
    "\n",
    "import google.genai as genai\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import hashlib\n",
    "import json\n",
    "from typing import Dict, List, Tuple, Optional, Union\n",
    "from unittest import result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we need to initialize the client responsible for response and embedding generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "client = genai.Client(api_key=os.getenv(\"GEMINI_API_KEY\"))\n",
    "gemini_cache = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_documents(directory_path: str) -> List[str]:\n",
    "    documents = [] \n",
    "    for filename in os.listdir(directory_path):  \n",
    "        if filename.endswith(\".txt\"):  \n",
    "            with open(os.path.join(directory_path, filename), 'r', encoding='utf-8') as file:\n",
    "                documents.append(file.read())\n",
    "    return documents  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are currently using a `chunk_size` of `100` characters, but this can be easily changed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_into_chunks(documents: List[str], chunk_size: int = 100) -> List[str]:\n",
    "    chunks = []  \n",
    "    for doc in documents:  \n",
    "        words = doc.split()  \n",
    "        for i in range(0, len(words), chunk_size):\n",
    "            chunk = \" \".join(words[i:i + chunk_size]) \n",
    "            chunks.append(chunk) \n",
    "    return chunks "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_text(text: str) -> str:\n",
    "    text = text.lower()\n",
    "    text = ''.join(char for char in text if char.isalnum() or char.isspace())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_chunks(chunks: List[str]) -> List[str]:\n",
    "    return [preprocess_text(chunk) for chunk in chunks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "directory_path = \"data\"\n",
    "documents = load_documents(directory_path)\n",
    "chunks = split_into_chunks(documents)\n",
    "preprocessed_chunks = preprocess_chunks(chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: quantum computing principles progress and possibil ... \n",
      "--------------------------------------------------\n",
      "Chunk 2: richard feynman proposed the idea of quantum compu ... \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(f\"Chunk {i+1}: {preprocessed_chunks[i][:50]} ... \")\n",
    "    print(\"-\" * 50)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _cache_key(prompt: str, model: str, **kwargs) -> str:\n",
    "    config_str = json.dumps(kwargs, sort_keys=True)\n",
    "    key_raw = f\"{model}:{prompt}:{config_str}\"\n",
    "    return hashlib.sha256(key_raw.encode(\"utf-8\")).hexdigest()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Document Embedding Generation\n",
    "\n",
    "The embedding model we are using is `gemini-embedding-001`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings_batch(chunks_batch: List[str]) -> List[List[float]]:\n",
    "    result = client.models.embed_content(\n",
    "        model=\"gemini-embedding-001\",\n",
    "        contents=chunks_batch\n",
    "    )\n",
    "\n",
    "    return [embedding.values for embedding in result.embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_embeddings(chunks: List[str], batch_size: int = 10) -> np.ndarray:\n",
    "    all_embeddings = []\n",
    "\n",
    "    for i in range(0, len(chunks), batch_size):\n",
    "        batch = chunks[i:i + batch_size]\n",
    "        embeddings = generate_embeddings_batch(batch)\n",
    "        all_embeddings.extend(embeddings)\n",
    "\n",
    "    return np.array(all_embeddings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_embeddings(embeddings: np.ndarray, output_file: str) -> None:\n",
    "    serialized = embeddings.tolist()\n",
    "\n",
    "    with open(output_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(serialized, file, indent=2)\n",
    "\n",
    "    print(f\"âœ… Saved {len(serialized)} embeddings to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Saved 9 embeddings to embeddings.json\n"
     ]
    }
   ],
   "source": [
    "preprocessed_chunks = preprocess_chunks(chunks)\n",
    "embeddings = generate_embeddings(preprocessed_chunks)\n",
    "save_embeddings(embeddings, \"embeddings.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vector Store Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector_store: dict[int, dict[str, object]] = {}\n",
    "\n",
    "def add_to_vector_store(embeddings: np.ndarray, chunks: List[str]) -> None:\n",
    "    for embedding, chunk in zip(embeddings, chunks):\n",
    "        vector_store[len(vector_store)] = {\"embedding\": embedding, \"chunk\": chunk}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simple Retrieval Implementation\n",
    "\n",
    "We do know for retrieving the most similar text chunks to a given query, we can use the cosine similarity between the query embedding and the embeddings of all text chunks. The higher the cosine similarity, the more similar the text chunks are. We can then sort the chunks based on their similarity scores and return the top-k most similar chunks.\n",
    "    \n",
    "So, let's implement a simple cosine similarity-based retrieval function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "tex"
    }
   },
   "source": [
    "The cosine similarity between two vectors $A$ and $B$ is calculated as:\n",
    "\n",
    "$$\\text{cosine similarity} = \\frac{A \\cdot B}{||A|| \\times ||B||} = \\frac{\\sum_{i=1}^{n} A_i B_i}{\\sqrt{\\sum_{i=1}^{n} A_i^2} \\times \\sqrt{\\sum_{i=1}^{n} B_i^2}}$$\n",
    "\n",
    "Where:\n",
    "- $A \\cdot B$ is the dot product of vectors $A$ and $B$\n",
    "- $||A||$ and $||B||$ are the Euclidean norms (magnitudes) of the vectors\n",
    "- $n$ is the dimension of the vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cosine_similarity(vec1: np.ndarray, vec2: np.ndarray) -> float:\n",
    "    dot_product = np.dot(vec1, vec2)\n",
    "    norm_vec1 = np.linalg.norm(vec1)\n",
    "    norm_vec2 = np.linalg.norm(vec2)\n",
    "    return dot_product / (norm_vec1 * norm_vec2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we calculate the cosine similarity between a query and all the chunks, we can perform a similarity search. Based on the `top_k` parameter, we retrieve the top k most similar chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similarity_search(query_embedding: np.ndarray, top_k: int = 5) -> List[str]:\n",
    "    similarities = []\n",
    "\n",
    "    for key, value in vector_store.items():\n",
    "        similarity = cosine_similarity(query_embedding, value[\"embedding\"])\n",
    "        similarities.append((key, similarity))\n",
    "\n",
    "    similarities = sorted(similarities, key=lambda x: x[1], reverse=True)\n",
    "    return [vector_store[key][\"chunk\"] for key, _ in similarities[:top_k]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once we have the similarity search function ready, we can simply code a retrieval function on top of it that will provide the relevant chunks based on the query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retrieve_relevant_chunks(query_text: str, top_k: int = 5) -> List[str]:\n",
    "    query_embedding = generate_embeddings([query_text])[0]\n",
    "    relevant_chunks = similarity_search(query_embedding, top_k=top_k)\n",
    "    return relevant_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have implemented all the functions for retrieval, we can proceed to test the retrieval system with a sample query."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk 1: quantum computing principles progress and possibil ... \n",
      "--------------------------------------------------\n",
      "Chunk 2: richard feynman proposed the idea of quantum compu ... \n",
      "--------------------------------------------------\n",
      "Chunk 3: where Î± and Î² are complex numbers satisfying Î±Â²  Î² ... \n",
      "--------------------------------------------------\n",
      "Chunk 4: process information through quantum gates analogou ... \n",
      "--------------------------------------------------\n",
      "Chunk 5: simple representation of an entangled state is the ... \n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "add_to_vector_store(embeddings, preprocessed_chunks)\n",
    "\n",
    "query_text = \"What is Quantum Computing?\"\n",
    "relevant_chunks = retrieve_relevant_chunks(query_text)\n",
    "\n",
    "for idx, chunk in enumerate(relevant_chunks):\n",
    "    print(f\"Chunk {idx + 1}: {chunk[:50]} ... \")\n",
    "    print(\"-\" * 50) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LLM Response Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_prompt(query: str, context_chunks: List[str]) -> str:\n",
    "    context = \"\\n\".join(context_chunks)\n",
    "    \n",
    "    system_message = (\n",
    "        \"You are a helpful assistant. Only use the provided context to answer the question. \"\n",
    "        \"If the context doesn't contain the information needed, say 'I don't have enough information to answer this question.'\"\n",
    "    )\n",
    "    \n",
    "    prompt = f\"System: {system_message}\\n\\nContext:\\n{context}\\n\\nQuestion:\\n{query}\\n\\nAnswer:\"\n",
    "    return prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_response(\n",
    "    prompt: str,\n",
    "    model: str = \"gemini-2.0-flash\",\n",
    "    max_output_tokens: int = 512,\n",
    "    temperature: float = 1.0,\n",
    "    top_p: float = 0.9,\n",
    "    top_k: int = 50\n",
    ") -> str:\n",
    "    \n",
    "    cache_key = _cache_key(\n",
    "        prompt,\n",
    "        model,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature,\n",
    "        top_p=top_p,\n",
    "        top_k=top_k\n",
    "    )\n",
    "\n",
    "    # Check cache first\n",
    "    if cache_key in gemini_cache:\n",
    "        return gemini_cache[cache_key]\n",
    "\n",
    "    # Call Gemini API\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=prompt,\n",
    "        config={\n",
    "            \"temperature\": temperature,\n",
    "            \"top_p\": top_p,\n",
    "            \"top_k\": top_k,\n",
    "            \"max_output_tokens\": max_output_tokens,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    # Extract text safely\n",
    "    output = response.text.strip() if hasattr(response, \"text\") else str(response)\n",
    "\n",
    "    # Store in cache\n",
    "    gemini_cache[cache_key] = output\n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic RAG Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def basic_rag_pipeline(query: str) -> str:\n",
    "    relevant_chunks: List[str] = retrieve_relevant_chunks(query)\n",
    "    prompt: str = construct_prompt(query, relevant_chunks)\n",
    "    response: str = generate_response(prompt)\n",
    "    return response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate the basic RAG pipeline\n",
    "\n",
    "Now that we have coded the basic RAG pipeline, we can use it for evaluation. Our evaluation queries contain different targeted segments, such as `factual_queries` and `complex_nature`. We are going to test the factual knowledge of our RAG pipeline.\n",
    "\n",
    "Let's load our evaluation queries and their expected answers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sample Query: What is the mathematical representation of a qubit in superposition?\n",
      "\n",
      "Expected Answer: |ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©, where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1, representing the probability amplitudes for measuring the qubit in state |0âŸ© or |1âŸ© respectively.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "with open('data/val.json', 'r') as file:\n",
    "    validation_data = json.load(file)\n",
    "\n",
    "sample_query = validation_data['basic_factual_questions'][0]['question']  # Extract the query text\n",
    "expected_answer = validation_data['basic_factual_questions'][0]['answer']  # Extract the ground truth answer\n",
    "\n",
    "print(f\"Sample Query: {sample_query}\\n\")\n",
    "print(f\"Expected Answer: {expected_answer}\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test the basic RAG pipeline with this eval query and see how well it performs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running the Retrieval-Augmented Generation (RAG) pipeline...\n",
      "ðŸ“¥ Query: What is the mathematical representation of a qubit in superposition?\n",
      "\n",
      "ðŸ¤– AI Response:\n",
      "--------------------------------------------------\n",
      "Ïˆ = Î±|0âŸ© + Î²|1âŸ©\n",
      "where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1 representing the probability amplitudes for measuring the qubit in state 0 or 1 respectively\n",
      "--------------------------------------------------\n",
      "âœ… Ground Truth Answer:\n",
      "--------------------------------------------------\n",
      "|ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©, where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1, representing the probability amplitudes for measuring the qubit in state |0âŸ© or |1âŸ© respectively.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "print(\"ðŸ” Running the Retrieval-Augmented Generation (RAG) pipeline...\")\n",
    "print(f\"ðŸ“¥ Query: {sample_query}\\n\")\n",
    "\n",
    "response = basic_rag_pipeline(sample_query)\n",
    "\n",
    "print(\"ðŸ¤– AI Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.strip())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "print(\"âœ… Ground Truth Answer:\")\n",
    "print(\"-\" * 50)\n",
    "print(expected_answer)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement a Reinforcement Learning-based RAG pipeline to address these shortcomings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "julia"
    }
   },
   "source": [
    "## Reinforcement Learning for RAG\n",
    "\n",
    "Reinforcement Learning (RL) is a type of machine learning where an agent learns to make decisions by taking actions in an environment to maximize some notion of cumulative reward. Unlike supervised learning, the agent is not explicitly told which actions to take, but instead must discover which actions yield the most reward through trial and error.\n",
    "\n",
    "Follow are the main components of a reinforcement learning system:\n",
    "\n",
    "1. **Agent**: The learner or decision-maker\n",
    "2. **Environment**: The world with which the agent interacts\n",
    "3. **State (S)**: The current situation of the agent in the environment\n",
    "4. **Action (A)**: A set of possible moves the agent can make\n",
    "5. **Reward (R)**: Feedback from the environment after each action\n",
    "6. **Policy (Ï€)**: Strategy that the agent follows to determine the next action\n",
    "\n",
    "The goal in reinforcement learning is to learn a policy Ï€ that maximizes the expected cumulative reward:\n",
    "\n",
    "$$\\pi^* = \\arg\\max_\\pi \\mathbb{E}\\left[ \\sum_{t=0}^{T} \\gamma^t R_t \\right]$$\n",
    "\n",
    "Where:\n",
    "- $\\pi^*$ is the optimal policy\n",
    "- $\\gamma$ is the discount factor (0 â‰¤ Î³ â‰¤ 1)\n",
    "- $R_t$ is the reward at time step t\n",
    "- $T$ is the final time step\n",
    "\n",
    "In the context of RAG systems, reinforcement learning can be used to:\n",
    "- Improve retrieval by learning which documents are most helpful\n",
    "- Refine prompt construction based on user feedback\n",
    "- Optimize the generation process by learning from successful responses"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## State, Action Space, and Reward Methodology\n",
    "\n",
    "The very first step when coding an RL algorithm is to define three things:\n",
    "\n",
    "- **State**: It is the current situation of the environment. In our case, the initial state is our simple RAG pipeline (query, context, response).\n",
    "- **Action Space**: It is the decision that the agent takes based on the state. In our case, the actions can include changing the model, modifying the context, altering the query, etc.\n",
    "- **Reward**: It is the feedback that the agent receives after taking an action. In our case, the reward can be the similarity between the generated response and the ground truth answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our state will be changing constantly as we perform training. For that, we need to save the state after each `training episode` so that our RL agent can learn from it and avoid making the same mistakes again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_state(\n",
    "    query: str, \n",
    "    context_chunks: List[str], \n",
    "    rewritten_query: str = None, \n",
    "    previous_responses: List[str] = None, \n",
    "    previous_rewards: List[float] = None\n",
    ") -> dict:\n",
    "    \n",
    "    state = {\n",
    "        \"original_query\": query,                                    \n",
    "        \"current_query\": rewritten_query if rewritten_query else query,  \n",
    "        \"context\": context_chunks,                                \n",
    "        \"previous_responses\": previous_responses if previous_responses else [],  \n",
    "        \"previous_rewards\": previous_rewards if previous_rewards else []         \n",
    "    }\n",
    "    return state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have defined the state representation for the RL agent, including the user query, retrieved context chunks, rewritten query (if any), and histories of responses and rewards. This state will guide the agent in generating better responses. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next we need to define the action space for the reinforcement learning agent. The action space consists of the set of possible actions that the agent can take at each step. In this case, we define four actions:\n",
    "- `rewrite_query`: Reformulate the original query to improve retrieval\n",
    "- `expand_context`: Retrieve additional context chunks\n",
    "- `filter_context`: Remove irrelevant context chunks\n",
    "- `generate_response`: Generate a response based on the current query and context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def define_action_space() -> List[str]:\n",
    "    actions = [\"rewrite_query\", \"expand_context\", \"filter_context\", \"generate_response\"]\n",
    "    return actions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Obviously, when our RL agent takes an action, it will be based on the current state and the action space. It will be rewarded based on the quality of the response generated by the RAG pipeline. The reward function will be based on the cosine similarity between the generated response and the ground truth answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reward(response: str, ground_truth: str) -> float:\n",
    "    response_embedding = generate_embeddings([response])[0]\n",
    "    ground_truth_embedding = generate_embeddings([ground_truth])[0]\n",
    "    \n",
    "    similarity = cosine_similarity(response_embedding, ground_truth_embedding)\n",
    "    return similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our goal is to maximize the reward by generating responses that are similar to the ground truth answer. Higher reward values indicate that the generated response is more aligned with the expected answer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Action Function Logic\n",
    "\n",
    "Now that we have defined the action space, we need to implement the logic for each action. This logic will determine how the RAG pipeline should be modified based on the action taken by the RL agent.\n",
    "\n",
    "Just to revisit, the four actions are:\n",
    "- `rewrite_query`: Reformulate the original query to improve retrieval\n",
    "- `expand_context`: Retrieve additional context chunks\n",
    "- `filter_context`: Remove irrelevant context chunks\n",
    "- `generate_response`: Generate a response based on the current query and context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create our first action logic for the agent. The first action we will implement is the `rewrite_query` action, which involves reformulating the original user query to improve retrieval performance. This action is crucial for enhancing the relevance of the retrieved context and generating more accurate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rewrite_query(\n",
    "    query: str,\n",
    "    context_chunks: List[str],\n",
    "    model: str = \"gemini-2.0-flash\",\n",
    "    max_output_tokens: int = 256,\n",
    "    temperature: float = 0.3\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    Uses Gemini to rewrite a query for better retrieval effectiveness with caching.\n",
    "    \"\"\"\n",
    "    rewrite_prompt = f\"\"\"\n",
    "    You are a query optimization assistant. Your task is to rewrite the given query\n",
    "    to make it more effective for retrieving relevant information.\n",
    "    Consider the following context snippets for inspiration.\n",
    "\n",
    "    Original Query:\n",
    "    {query}\n",
    "\n",
    "    Context Snippets:\n",
    "    {' '.join(context_chunks[:3])}\n",
    "\n",
    "    Provide only the rewritten query:\n",
    "    \"\"\"\n",
    "\n",
    "    # Generate cache key\n",
    "    cache_key = _cache_key(\n",
    "        rewrite_prompt,\n",
    "        model,\n",
    "        max_output_tokens=max_output_tokens,\n",
    "        temperature=temperature\n",
    "    )\n",
    "\n",
    "    # Return cached result if available\n",
    "    if cache_key in gemini_cache:\n",
    "        return gemini_cache[cache_key]\n",
    "\n",
    "    # Call Gemini API\n",
    "    response = client.models.generate_content(\n",
    "        model=model,\n",
    "        contents=rewrite_prompt,\n",
    "        config={\n",
    "            \"temperature\": temperature,\n",
    "            \"max_output_tokens\": max_output_tokens\n",
    "        }\n",
    "    )\n",
    "\n",
    "    rewritten_query = response.text.strip() if hasattr(response, \"text\") else str(response)\n",
    "\n",
    "    # Store in cache\n",
    "    gemini_cache[cache_key] = rewritten_query\n",
    "\n",
    "    return rewritten_query"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This action is crucial for enhancing the relevance of the retrieved context and generating more accurate responses."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's code our next action logic, which is to expand the context by retrieving additional chunks. We will use the existing function `retrieve_relevant_chunks` to get more context chunks and then filter out any duplicates from the current context. We will limit the number of new chunks to be added to the context to a specified top_k value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def expand_context(query: str, current_chunks: List[str], top_k: int = 3) -> List[str]:\n",
    "    additional_chunks = retrieve_relevant_chunks(query, top_k=top_k + len(current_chunks))\n",
    "    \n",
    "    new_chunks = []\n",
    "    for chunk in additional_chunks:\n",
    "        if chunk not in current_chunks:\n",
    "            new_chunks.append(chunk)\n",
    "    \n",
    "    expanded_context = current_chunks + new_chunks[:top_k]\n",
    "    return expanded_context"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to filter the context to keep only the most relevant chunks for the query. This filtering step is crucial to ensure that the context provided to the language model is concise and focused on the most relevant information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_context(query: str, context_chunks: List[str]) -> List[str]:\n",
    "    if not context_chunks:\n",
    "        return []\n",
    "        \n",
    "    query_embedding = generate_embeddings([query])[0]\n",
    "    chunk_embeddings = [generate_embeddings([chunk])[0] for chunk in context_chunks]\n",
    "    \n",
    "    relevance_scores = []\n",
    "    for chunk_embedding in chunk_embeddings:\n",
    "        score = cosine_similarity(query_embedding, chunk_embedding)\n",
    "        relevance_scores.append(score)\n",
    "    \n",
    "    sorted_chunks = [x for _, x in sorted(zip(relevance_scores, context_chunks), reverse=True)]\n",
    "    filtered_chunks = sorted_chunks[:min(5, len(sorted_chunks))]\n",
    "    return filtered_chunks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This action will help the agent explore more information relevant to the query."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policy Network\n",
    "\n",
    "Previously, we defined our state, actions, and reward logic. Next, we need to create a policy network that will select an action based on the current state.\n",
    "\n",
    "A policy network is a function that takes the current state and the action space as input and returns the selected action based on the state.\n",
    "\n",
    "The policy network can use a simple heuristic to select an action based on the current state. For example, if there are no previous responses, the policy network can prioritize rewriting the query. If the context has too many chunks, the policy network can choose to filter the context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_network(\n",
    "    state: dict, \n",
    "    action_space: List[str], \n",
    "    epsilon: float = 0.2\n",
    ") -> str:\n",
    "    \n",
    "    if np.random.random() < epsilon:\n",
    "        action = np.random.choice(action_space)\n",
    "    else: \n",
    "        if len(state[\"previous_responses\"]) == 0:\n",
    "            action = \"rewrite_query\"\n",
    "        elif state[\"previous_rewards\"] and max(state[\"previous_rewards\"]) < 0.7:\n",
    "            action = \"expand_context\"\n",
    "        elif len(state[\"context\"]) > 5:\n",
    "            action = \"filter_context\"\n",
    "        else:\n",
    "            action = \"generate_response\"\n",
    "    \n",
    "    return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our policy network works like this:\n",
    "- If there are no previous responses, prioritize rewriting the query.\n",
    "- If there are previous responses but the rewards are low, try expanding the context.\n",
    "- If the context has too many chunks, try filtering the context.\n",
    "- Otherwise, generate a response."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Single RL Step\n",
    "\n",
    "We have coded an important component of the RL pipeline. For any developer who has done any kind of training, there exists a training loop where each iteration is a single step in which the RL agent takes an action, rewards are calculated, states are updated, and so on. So, we need to code a single step of our training loop. Let's do that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def rl_step(\n",
    "    state: dict, \n",
    "    action_space: List[str], \n",
    "    ground_truth: str\n",
    ") -> tuple[dict, str, float, str]:\n",
    "\n",
    "    action: str = policy_network(state, action_space)\n",
    "    response: str = None  \n",
    "    reward: float = 0  \n",
    "\n",
    "    if action == \"rewrite_query\":\n",
    "        rewritten_query: str = rewrite_query(state[\"original_query\"], state[\"context\"])\n",
    "        state[\"current_query\"] = rewritten_query  \n",
    "        new_context: List[str] = retrieve_relevant_chunks(rewritten_query)\n",
    "        state[\"context\"] = new_context  \n",
    "\n",
    "    elif action == \"expand_context\":\n",
    "        expanded_context: List[str] = expand_context(state[\"current_query\"], state[\"context\"])\n",
    "        state[\"context\"] = expanded_context \n",
    "\n",
    "    elif action == \"filter_context\":\n",
    "        filtered_context: List[str] = filter_context(state[\"current_query\"], state[\"context\"])\n",
    "        state[\"context\"] = filtered_context  \n",
    "\n",
    "    elif action == \"generate_response\":\n",
    "        prompt: str = construct_prompt(state[\"current_query\"], state[\"context\"])\n",
    "        response: str = generate_response(prompt)\n",
    "        reward: float = calculate_reward(response, ground_truth)\n",
    "        state[\"previous_responses\"].append(response)\n",
    "        state[\"previous_rewards\"].append(reward)\n",
    "\n",
    "    return state, action, reward, response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our single step function, we first select an action using the policy network. The policy network uses an epsilon-greedy strategy to balance exploration and exploitation. If the random number is less than epsilon, we choose a random action from the action space for exploration. Otherwise, we select the best action based on the current state using a simple heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Parameters and Policy Update\n",
    "\n",
    "We need to define some training parameters for our training loop and also define a function to update the policy based on the rewards received.\n",
    "\n",
    "Though the training parameters function is **optional**, it can be used for advanced implementations of the RL pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_training_params() -> Dict[str, Union[float, int]]:\n",
    "    params = {\n",
    "        \"learning_rate\": 0.01,  \n",
    "        \"num_episodes\": 100,\n",
    "        \"discount_factor\": 0.99\n",
    "    }\n",
    "    return params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to how our state changes after each step in the RL process, the policy also needs to be updated based on the rewards received. The update_policy function takes the current policy, state, action, reward, and learning rate as input and returns the updated policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_policy(\n",
    "    policy: Dict[str, Dict[str, Union[float, str]]], \n",
    "    state: Dict[str, object], \n",
    "    action: str, \n",
    "    reward: float, \n",
    "    learning_rate: float\n",
    ") -> Dict[str, Dict[str, Union[float, str]]]:\n",
    "    \n",
    "    policy[state[\"query\"]] = {\n",
    "        \"action\": action,\n",
    "        \"reward\": reward\n",
    "    }\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above `update_policy` logic, we store the action taken and the reward received for each query in the policy dictionary. In a more advanced RL algorithm, the policy update would involve more sophisticated methods such as policy gradients or Q-learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we need to implement progress tracking logic to monitor the training process. This will help us understand how the model is learning and improving over time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def track_progress(\n",
    "    episode: int, \n",
    "    reward: float, \n",
    "    rewards_history: List[float]\n",
    ") -> List[float]:\n",
    "    \n",
    "    rewards_history.append(reward)\n",
    "    print(f\"Episode {episode}: Reward = {reward}\")\n",
    "    return rewards_history"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Loop\n",
    "\n",
    "Now that we have coded every part of the training loop, we can put it all together in a single function that implements the training loop for the RL-enhanced RAG system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_loop(\n",
    "    query_text: str, \n",
    "    ground_truth: str, \n",
    "    params: Optional[Dict[str, Union[float, int]]] = None\n",
    ") -> Tuple[Dict[str, Dict[str, Union[float, str]]], List[float], List[List[str]], Optional[str]]:\n",
    "    \n",
    "    if params is None:\n",
    "        params = initialize_training_params()\n",
    "    \n",
    "    rewards_history: List[float] = []  \n",
    "    actions_history: List[List[str]] = []  \n",
    "    policy: Dict[str, Dict[str, Union[float, str]]] = {}  \n",
    "    action_space: List[str] = define_action_space() \n",
    "    best_response: Optional[str] = None \n",
    "    best_reward: float = -1  \n",
    "    \n",
    "    simple_response: str = basic_rag_pipeline(query_text)\n",
    "    simple_reward: float = calculate_reward(simple_response, ground_truth)\n",
    "    print(f\"Simple RAG reward: {simple_reward:.4f}\")\n",
    "\n",
    "    for episode in range(params[\"num_episodes\"]):\n",
    "        context_chunks: List[str] = retrieve_relevant_chunks(query_text)\n",
    "        state: Dict[str, object] = define_state(query_text, context_chunks)\n",
    "        episode_reward: float = 0  \n",
    "        episode_actions: List[str] = []  \n",
    "        \n",
    "        for step in range(10):\n",
    "            state, action, reward, response = rl_step(state, action_space, ground_truth)\n",
    "            episode_actions.append(action) \n",
    "            \n",
    "            if response:\n",
    "                episode_reward = reward  \n",
    "                \n",
    "                if reward > best_reward:\n",
    "                    best_reward = reward\n",
    "                    best_response = response\n",
    "                \n",
    "                break \n",
    "        \n",
    "        rewards_history.append(episode_reward)\n",
    "        actions_history.append(episode_actions)\n",
    "        \n",
    "        if episode % 5 == 0:\n",
    "            print(f\"Episode {episode}: Reward = {episode_reward:.4f}, Actions = {episode_actions}\")\n",
    "    \n",
    "    improvement: float = best_reward - simple_reward\n",
    "    print(f\"\\nTraining completed:\")\n",
    "    print(f\"Simple RAG reward: {simple_reward:.4f}\")\n",
    "    print(f\"Best RL-enhanced RAG reward: {best_reward:.4f}\")\n",
    "    print(f\"Improvement: {improvement:.4f} ({improvement * 100:.2f}%)\")\n",
    "\n",
    "    return policy, rewards_history, actions_history, best_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take the input query text, the expected ground truth answer, and optionally some training parameters. It will return the updated policy, a list of rewards received in each episode, a list of actions taken in each episode, and the best response generated during training.\n",
    "\n",
    "In more detail, the `training_loop` function will:\n",
    "- Initialize training parameters if not provided.\n",
    "- Get the initial performance from the simple RAG pipeline for comparison.\n",
    "- Start the training loop for the specified number of episodes.\n",
    "- Perform a single RL step in each episode.\n",
    "- Update rewards and actions history for each episode.\n",
    "- Print progress every 5 episodes.\n",
    "- Compare the best RL-enhanced RAG reward with the simple RAG reward.\n",
    "- Return the updated policy, rewards history, actions history, and the best response generated during training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance Comparison Logic\n",
    "\n",
    "Although we can manually compare the simple RAG pipeline with the RL-based RAG pipeline, a function can definitely help us in this regard. So, let's define a function to compare the performance of the simple RAG pipeline with the RL-enhanced RAG pipeline."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compare_rag_approaches(query_text: str, ground_truth: str) -> Tuple[str, str, float, float]:\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Query: {query_text}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    simple_response: str = basic_rag_pipeline(query_text)\n",
    "    simple_similarity: float = calculate_reward(simple_response, ground_truth)\n",
    "    \n",
    "    print(\"\\nSimple RAG Output:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(simple_response)\n",
    "    print(f\"Similarity to ground truth: {simple_similarity:.4f}\")\n",
    "    \n",
    "    print(\"\\nTraining RL-enhanced RAG model...\")\n",
    "    params: Dict[str, float | int] = initialize_training_params()\n",
    "    params[\"num_episodes\"] = 5\n",
    "    \n",
    "    _, rewards_history, actions_history, best_rl_response = training_loop(\n",
    "        query_text, ground_truth, params\n",
    "    )\n",
    "    \n",
    "    if best_rl_response is None:\n",
    "        context_chunks: List[str] = retrieve_relevant_chunks(query_text)\n",
    "        prompt: str = construct_prompt(query_text, context_chunks)\n",
    "        best_rl_response: str = generate_response(prompt)\n",
    "    \n",
    "    rl_similarity: float = calculate_reward(best_rl_response, ground_truth)\n",
    "    \n",
    "    print(\"\\nRL-enhanced RAG Output:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(best_rl_response)\n",
    "    print(f\"Similarity to ground truth: {rl_similarity:.4f}\")\n",
    "    \n",
    "    improvement: float = rl_similarity - simple_similarity\n",
    "    \n",
    "    print(\"\\nEvaluation Results:\")\n",
    "    print(\"-\" * 40)\n",
    "    print(f\"Simple RAG similarity to ground truth: {simple_similarity:.4f}\")\n",
    "    print(f\"RL-enhanced RAG similarity to ground truth: {rl_similarity:.4f}\")\n",
    "    print(f\"Improvement: {improvement * 100:.2f}%\")\n",
    "    \n",
    "    if len(rewards_history) > 1:\n",
    "        try:\n",
    "            import matplotlib.pyplot as plt\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(rewards_history)\n",
    "            plt.title('Reward History During RL Training')\n",
    "            plt.xlabel('Episode')\n",
    "            plt.ylabel('Reward')\n",
    "            plt.grid(True)\n",
    "            plt.show()\n",
    "        except ImportError:\n",
    "            print(\"Matplotlib not available for plotting rewards\")\n",
    "    \n",
    "    return simple_response, best_rl_response, simple_similarity, rl_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So our performance comparison logic is not very complicated but is based on 4 steps:\n",
    "1. Generate a response using the simple RAG pipeline.\n",
    "2. Train the RL-enhanced RAG model using the training loop.\n",
    "3. Evaluate and compare the results.\n",
    "4. Plot the reward history (if available)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluation Framework (**Optional**)\n",
    "\n",
    "This step is optional but in case you want to evaluate all the eval queries in the validation data, you can use the following code.\n",
    "\n",
    "First, to check the relevance of the retrieved chunks and the ground truth, we need to have a function that evaluates the relevance of the retrieved chunks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_relevance(retrieved_chunks: List[str], ground_truth_chunks: List[str]) -> float:\n",
    "    relevance_scores: List[float] = [] \n",
    "\n",
    "    for retrieved, ground_truth in zip(retrieved_chunks, ground_truth_chunks):\n",
    "        relevance: float = cosine_similarity(\n",
    "            generate_embeddings([retrieved])[0],\n",
    "            generate_embeddings([ground_truth])[0]\n",
    "        )\n",
    "        relevance_scores.append(relevance)\n",
    "\n",
    "    return np.mean(relevance_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To evaluate the accuracy of the generated responses, we can use the cosine similarity between the embeddings of the generated responses and the ground truth. So let's define a function to evaluate the accuracy of the responses based on this similarity metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_accuracy(responses: List[str], ground_truth_responses: List[str]) -> float:\n",
    "    accuracy_scores: List[float] = []  \n",
    "\n",
    "    for response, ground_truth in zip(responses, ground_truth_responses):\n",
    "        accuracy: float = cosine_similarity(\n",
    "            generate_embeddings([response])[0],\n",
    "            generate_embeddings([ground_truth])[0]\n",
    "        )\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "    return np.mean(accuracy_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also need to measure the response quality and assign a relevant score for it to be used in the reinforcement learning process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_response_quality(responses: List[str]) -> float:\n",
    "    quality_scores: List[float] = []  \n",
    "\n",
    "    for response in responses:\n",
    "        quality: float = len(response.split()) / 100\n",
    "        quality_scores.append(min(quality, 1.0)) \n",
    "\n",
    "    return np.mean(quality_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then we can evaluate the performance of the RL-enhanced RAG model on the validation dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_rag_performance(\n",
    "    queries: List[str], \n",
    "    ground_truth_chunks: List[str], \n",
    "    ground_truth_responses: List[str]\n",
    ") -> Dict[str, float]:\n",
    "    \n",
    "    relevance_scores: List[float] = []\n",
    "    accuracy_scores: List[float] = []\n",
    "    quality_scores: List[float] = []\n",
    "\n",
    "    # Iterate through each query and its corresponding ground truth data\n",
    "    for query, ground_truth_chunk, ground_truth_response in zip(queries, ground_truth_chunks, ground_truth_responses):\n",
    "        # Retrieve relevant chunks for the query\n",
    "        retrieved_chunks: List[str] = retrieve_relevant_chunks(query)\n",
    "        \n",
    "        # Evaluate the relevance of the retrieved chunks compared to the ground truth chunk\n",
    "        relevance: float = evaluate_relevance(retrieved_chunks, [ground_truth_chunk])\n",
    "        relevance_scores.append(relevance)\n",
    "\n",
    "        # Generate a response using the basic RAG pipeline\n",
    "        response: str = basic_rag_pipeline(query)\n",
    "        \n",
    "        # Evaluate the accuracy of the generated response compared to the ground truth response\n",
    "        accuracy: float = evaluate_accuracy([response], [ground_truth_response])\n",
    "        accuracy_scores.append(accuracy)\n",
    "\n",
    "        # Evaluate the quality of the generated response\n",
    "        quality: float = evaluate_response_quality([response])\n",
    "        quality_scores.append(quality)\n",
    "\n",
    "    # Calculate the average scores for each metric\n",
    "    avg_relevance: float = np.mean(relevance_scores)\n",
    "    avg_accuracy: float = np.mean(accuracy_scores)\n",
    "    avg_quality: float = np.mean(quality_scores)\n",
    "\n",
    "    # Return the average scores as a dictionary\n",
    "    return {\n",
    "        \"average_relevance\": avg_relevance,\n",
    "        \"average_accuracy\": avg_accuracy,\n",
    "        \"average_quality\": avg_quality\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating (RL vs Simple) RAG\n",
    "\n",
    "Ah, the moment of truth! Let's evaluate the performance of the simple RAG pipeline against the RL-enhanced RAG pipeline on our factual query, where the simple RAG previously failed to provide the correct answer. Let's see if the RL-enhanced RAG pipeline can perform better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's revisit our evaluation query and see what the simple RAG pipeline generates for it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸ” Running the Retrieval-Augmented Generation (RAG) pipeline...\n",
      "ðŸ“¥ Query: What is the mathematical representation of a qubit in superposition?\n",
      "\n",
      "ðŸ¤– AI Response:\n",
      "--------------------------------------------------\n",
      "Ïˆ = Î±|0âŸ© + Î²|1âŸ©\n",
      "where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1 representing the probability amplitudes for measuring the qubit in state 0 or 1 respectively\n",
      "--------------------------------------------------\n",
      "âœ… Ground Truth Answer:\n",
      "--------------------------------------------------\n",
      "|ÏˆâŸ© = Î±|0âŸ© + Î²|1âŸ©, where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1, representing the probability amplitudes for measuring the qubit in state |0âŸ© or |1âŸ© respectively.\n",
      "--------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Print a message to indicate the start of the RAG pipeline\n",
    "print(\"ðŸ” Running the Retrieval-Augmented Generation (RAG) pipeline...\")\n",
    "print(f\"ðŸ“¥ Query: {sample_query}\\n\")\n",
    "\n",
    "# Run the RAG pipeline and get the response\n",
    "response = basic_rag_pipeline(sample_query)\n",
    "\n",
    "# Print the response with better formatting\n",
    "print(\"ðŸ¤– AI Response:\")\n",
    "print(\"-\" * 50)\n",
    "print(response.strip())\n",
    "print(\"-\" * 50)\n",
    "\n",
    "# Print the ground truth answer for comparison\n",
    "print(\"âœ… Ground Truth Answer:\")\n",
    "print(\"-\" * 50)\n",
    "print(expected_answer)\n",
    "print(\"-\" * 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "Query: What is the mathematical representation of a qubit in superposition?\n",
      "================================================================================\n",
      "\n",
      "Simple RAG Output:\n",
      "----------------------------------------\n",
      "Ïˆ = Î±|0âŸ© + Î²|1âŸ©\n",
      "where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1 representing the probability amplitudes for measuring the qubit in state 0 or 1 respectively\n",
      "Similarity to ground truth: 0.9897\n",
      "\n",
      "Training RL-enhanced RAG model...\n",
      "Simple RAG reward: 0.9897\n",
      "Episode 0: Reward = 0.0000, Actions = ['rewrite_query', 'rewrite_query', 'rewrite_query', 'rewrite_query', np.str_('rewrite_query'), 'rewrite_query', 'rewrite_query', 'rewrite_query', np.str_('filter_context'), 'rewrite_query']\n",
      "\n",
      "Training completed:\n",
      "Simple RAG reward: 0.9897\n",
      "Best RL-enhanced RAG reward: 0.9920\n",
      "Improvement: 0.0023 (0.23%)\n",
      "\n",
      "RL-enhanced RAG Output:\n",
      "----------------------------------------\n",
      "Ïˆ = Î±|0âŸ© + Î²|1âŸ©\n",
      "where Î± and Î² are complex numbers satisfying |Î±|Â² + |Î²|Â² = 1, representing the probability amplitudes for measuring the qubit in state 0 or 1 respectively.\n",
      "Similarity to ground truth: 0.9920\n",
      "\n",
      "Evaluation Results:\n",
      "----------------------------------------\n",
      "Simple RAG similarity to ground truth: 0.9897\n",
      "RL-enhanced RAG similarity to ground truth: 0.9920\n",
      "Improvement: 0.23%\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x600 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "simple_response, rl_response, simple_sim, rl_sim = compare_rag_approaches(sample_query, expected_answer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can clearly see that the response generated by the RL-enhanced RAG model is more accurate and relevant compared to the simple RAG pipeline. The improvement in similarity to the ground truth is evident, indicating that the RL-enhanced model has learned to generate better responses through training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Comparison Results\n",
    "\n",
    "After implementing the RL algorithm, we can save the comparison results to check the performance of the RL implementation later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Results saved to rl_rag_results.json\n"
     ]
    }
   ],
   "source": [
    "results = {\n",
    "    \"query\": query_text, \n",
    "    \"ground_truth\": expected_answer, \n",
    "    \"simple_rag\": {\n",
    "        \"response\": simple_response,  \n",
    "        \"similarity\": float(simple_sim) \n",
    "    },\n",
    "    \"rl_rag\": {\n",
    "        \"response\": rl_response, \n",
    "        \"similarity\": float(rl_sim) \n",
    "    },\n",
    "    \"improvement\": float(rl_sim - simple_sim)  \n",
    "}\n",
    "\n",
    "with open('rl_rag_results.json', 'w') as f:\n",
    "    json.dump(results, f, indent=2)  \n",
    "\n",
    "print(\"\\nResults saved to rl_rag_results.json\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What can we conclude?\n",
    "\n",
    "- The performance of the simple RAG is lower compared to the RL-enhanced RAG on factual queries.\n",
    "- The RL-enhanced RAG achieved a % improvement in the similarity score within 5 episodes.\n",
    "- Further improvements can be achieved by:\n",
    "    - Training for more episodes.\n",
    "    - Tuning hyperparameters.\n",
    "- Time is a key constraint for training.\n",
    "- Parallel implementation of the RL algorithm can help reduce training time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
